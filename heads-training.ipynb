{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8182506,"sourceType":"datasetVersion","datasetId":4844638},{"sourceId":8183481,"sourceType":"datasetVersion","datasetId":4845348}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-04-21T10:36:43.250589Z","iopub.execute_input":"2024-04-21T10:36:43.250926Z","iopub.status.idle":"2024-04-21T10:36:44.076586Z","shell.execute_reply.started":"2024-04-21T10:36:43.250897Z","shell.execute_reply":"2024-04-21T10:36:44.075677Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/shtormzxc/50k_25cats_FINAL.pkl\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport pickle\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, roc_auc_score, classification_report\nfrom sklearn.preprocessing import LabelEncoder\nfrom transformers import BertModel, AutoTokenizer\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"execution":{"iopub.status.busy":"2024-04-21T10:36:47.571003Z","iopub.execute_input":"2024-04-21T10:36:47.571468Z","iopub.status.idle":"2024-04-21T10:36:55.375652Z","shell.execute_reply.started":"2024-04-21T10:36:47.571439Z","shell.execute_reply":"2024-04-21T10:36:55.374694Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"DATA = pd.read_pickle('/kaggle/input/shtormzxc/50k_25cats_FINAL.pkl')\nDATA","metadata":{"execution":{"iopub.status.busy":"2024-04-21T10:36:58.291746Z","iopub.execute_input":"2024-04-21T10:36:58.292767Z","iopub.status.idle":"2024-04-21T10:37:00.575780Z","shell.execute_reply.started":"2024-04-21T10:36:58.292725Z","shell.execute_reply":"2024-04-21T10:37:00.574939Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                                     text          topic\n347     источник рецепты леди mail ru ингредиенты пече...        рецепты\n338     выбор косметики и инвентаря для качественной о...           мода\n324057  европейские странычлены оэср с января по авгус...      экономика\n44746   российский футболист иван заборовский выступаю...          спорт\n392947  в суд направлено уголовное дело в отношении од...       общество\n...                                                   ...            ...\n330332  тем кто живет в ожидании повестки не стоит ухо...  забота о себе\n463     в этом сезоне серый  новый черный он встречалс...           мода\n321949  лидер партии яблоко сергей митрохин направил в...       общество\n1544    источник рецепты леди mail ru ингредиенты карт...        рецепты\n864     источник рецепты леди mail ru ингредиенты апел...        рецепты\n\n[49413 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>topic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>347</th>\n      <td>источник рецепты леди mail ru ингредиенты пече...</td>\n      <td>рецепты</td>\n    </tr>\n    <tr>\n      <th>338</th>\n      <td>выбор косметики и инвентаря для качественной о...</td>\n      <td>мода</td>\n    </tr>\n    <tr>\n      <th>324057</th>\n      <td>европейские странычлены оэср с января по авгус...</td>\n      <td>экономика</td>\n    </tr>\n    <tr>\n      <th>44746</th>\n      <td>российский футболист иван заборовский выступаю...</td>\n      <td>спорт</td>\n    </tr>\n    <tr>\n      <th>392947</th>\n      <td>в суд направлено уголовное дело в отношении од...</td>\n      <td>общество</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>330332</th>\n      <td>тем кто живет в ожидании повестки не стоит ухо...</td>\n      <td>забота о себе</td>\n    </tr>\n    <tr>\n      <th>463</th>\n      <td>в этом сезоне серый  новый черный он встречалс...</td>\n      <td>мода</td>\n    </tr>\n    <tr>\n      <th>321949</th>\n      <td>лидер партии яблоко сергей митрохин направил в...</td>\n      <td>общество</td>\n    </tr>\n    <tr>\n      <th>1544</th>\n      <td>источник рецепты леди mail ru ингредиенты карт...</td>\n      <td>рецепты</td>\n    </tr>\n    <tr>\n      <th>864</th>\n      <td>источник рецепты леди mail ru ингредиенты апел...</td>\n      <td>рецепты</td>\n    </tr>\n  </tbody>\n</table>\n<p>49413 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"len(DATA['topic'].value_counts().to_dict().keys())","metadata":{"execution":{"iopub.status.busy":"2024-04-21T10:37:03.307298Z","iopub.execute_input":"2024-04-21T10:37:03.308126Z","iopub.status.idle":"2024-04-21T10:37:03.327091Z","shell.execute_reply.started":"2024-04-21T10:37:03.308092Z","shell.execute_reply":"2024-04-21T10:37:03.326214Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"25"},"metadata":{}}]},{"cell_type":"code","source":"lbl_enc = LabelEncoder()\nDATA['topic'] = lbl_enc.fit_transform(DATA['topic'])","metadata":{"execution":{"iopub.status.busy":"2024-04-21T10:37:05.407229Z","iopub.execute_input":"2024-04-21T10:37:05.407560Z","iopub.status.idle":"2024-04-21T10:37:05.424901Z","shell.execute_reply.started":"2024-04-21T10:37:05.407537Z","shell.execute_reply":"2024-04-21T10:37:05.423940Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"DATA['topic'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-04-21T10:37:08.523164Z","iopub.execute_input":"2024-04-21T10:37:08.524004Z","iopub.status.idle":"2024-04-21T10:37:08.533615Z","shell.execute_reply.started":"2024-04-21T10:37:08.523968Z","shell.execute_reply":"2024-04-21T10:37:08.532562Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"topic\n24    2301\n19    2301\n16    2300\n10    2300\n23    2300\n4     2300\n0     2300\n5     2300\n17    2300\n2     2300\n15    2300\n21    2300\n9     2300\n7     2296\n8     2286\n22    2192\n13    2181\n6     2152\n20    2095\n18    2008\n3     1881\n14    1253\n11     839\n12     242\n1       86\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"device = 'cuda'\n\nclass ClassifierNN1(nn.Module):\n    def __init__(self, N_classes=25):\n        super(ClassifierNN1, self).__init__()\n        self.bert = BertModel.from_pretrained('gblssroman/mailru-bert-base').to(device)\n        self.tokenizer_bert = AutoTokenizer.from_pretrained('gblssroman/mailru-bert-base')\n        self.rubert = BertModel.from_pretrained('DeepPavlov/rubert-base-cased').to(device)\n        self.tokenizer_rubert = AutoTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')\n        \n        self.fc_bert = nn.Sequential(\n            nn.Linear(768, 256),\n            nn.BatchNorm1d(num_features=256),\n            nn.GELU()\n        )\n        self.fc_rubert = nn.Sequential(\n            nn.Linear(768, 256),\n            nn.BatchNorm1d(num_features=256),\n            nn.GELU()\n        )\n        self.fc = nn.Sequential(\n            nn.Linear(512, N_classes)\n        )\n        \n    def mean_pooling(self, model_output, attention_mask):\n        token_embeddings = model_output[0]  # each token embeds\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n        return sum_embeddings / sum_mask\n    \n    def freeze_berts(self):\n        for i in [self.bert, self.rubert]:\n            for param in i.parameters():\n                param.requires_grad = False\n\n    def forward(self, x):\n        y = self.tokenizer_bert(x, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n        z = self.tokenizer_rubert(x, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n        y_emb, z_emb = self.bert(**y), self.rubert(**z)\n        y_emb_pooled = self.mean_pooling(y_emb, y['attention_mask']).to(device)\n        z_emb_pooled = self.mean_pooling(z_emb, z['attention_mask']).to(device)\n        y_fc = self.fc_bert(y_emb_pooled)\n        z_fc = self.fc_rubert(z_emb_pooled)\n        yz = torch.cat((y_fc, z_fc), dim=1)\n        out = self.fc(yz)\n        return out\n    \nmodel1 = ClassifierNN1()\nmodel1.freeze_berts()\nmodel1.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T10:37:13.865726Z","iopub.execute_input":"2024-04-21T10:37:13.866432Z","iopub.status.idle":"2024-04-21T10:37:59.037896Z","shell.execute_reply.started":"2024-04-21T10:37:13.866398Z","shell.execute_reply":"2024-04-21T10:37:59.037033Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/647 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3971026cdc1c49048020999d8100a9ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"696c2f1a83a4400886665aca644c0696"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertModel were not initialized from the model checkpoint at gblssroman/mailru-bert-base and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.26k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5da4cba46ebb48bd90ffcadc9b39df68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/184k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d543083677f435dacb7be216301de87"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/416k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd2a73c423844318a4716b8d058820f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30c5fc22b7554ecd817900d7069bcb9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d7a5372d9f2490087947123512619c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2496ba5d22ec4840bb94cebd057629ea"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/24.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"141fee22b8d64e5cadac54c0926545e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/1.65M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8326421950d74f30989b2ec60cf828b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3e5d965efe34dae9107f6e5faa656c6"}},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"ClassifierNN1(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (rubert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (fc_bert): Sequential(\n    (0): Linear(in_features=768, out_features=256, bias=True)\n    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): GELU(approximate='none')\n  )\n  (fc_rubert): Sequential(\n    (0): Linear(in_features=768, out_features=256, bias=True)\n    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): GELU(approximate='none')\n  )\n  (fc): Sequential(\n    (0): Linear(in_features=512, out_features=25, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"class TextDataset(Dataset):\n    def __init__(self, text, labels):\n        self.text = text.to_numpy()\n        self.labels = torch.tensor(labels.to_numpy(), dtype=torch.float32)\n    def __getitem__(self, idx):\n        out = {}\n        out['text'] = self.text[idx]\n        out['labels'] = self.labels[idx]\n        return out\n    def __len__(self):\n        return len(self.labels)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T10:38:05.838215Z","iopub.execute_input":"2024-04-21T10:38:05.838569Z","iopub.status.idle":"2024-04-21T10:38:05.844810Z","shell.execute_reply.started":"2024-04-21T10:38:05.838537Z","shell.execute_reply":"2024-04-21T10:38:05.843844Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"X = DATA['text']\nY = DATA['topic']\nXtrain, Xval, Ytrain, Yval = train_test_split(X, Y, stratify=Y, test_size=0.25, random_state=666, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T10:38:07.863149Z","iopub.execute_input":"2024-04-21T10:38:07.863961Z","iopub.status.idle":"2024-04-21T10:38:07.899795Z","shell.execute_reply.started":"2024-04-21T10:38:07.863927Z","shell.execute_reply":"2024-04-21T10:38:07.898649Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 128\n\ntrainset = TextDataset(Xtrain, Ytrain)\nvalset = TextDataset(Xval, Yval)\ntrainloader = DataLoader(trainset, batch_size=BATCH_SIZE)\nvalloader = DataLoader(valset, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T10:38:13.116621Z","iopub.execute_input":"2024-04-21T10:38:13.117392Z","iopub.status.idle":"2024-04-21T10:38:13.127210Z","shell.execute_reply.started":"2024-04-21T10:38:13.117360Z","shell.execute_reply":"2024-04-21T10:38:13.126239Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 3\nLR = 6e-4\n\noptimizer = torch.optim.AdamW(params=model1.parameters(), lr=LR)\nscheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=0.7)\nloss = nn.CrossEntropyLoss()\nsfmax = nn.Softmax(dim=1)\n\nf1train, f1val = [], []\nlosstrain, lossval = [], []\n\nfor epoch in range(EPOCHS):\n    model1.train()\n    model1.freeze_berts()\n    \n    with tqdm(trainloader, unit='batch') as tepoch:\n        for batch in tepoch:\n            text = batch['text']\n            labels = batch['labels'].type(torch.LongTensor).to(device)\n            \n            optimizer.zero_grad()\n            logits = model1(text).to(device)\n            curr_loss = loss(logits, labels)\n            preds = torch.argmax(sfmax(logits), dim=1, keepdim=True).squeeze()\n            \n            f1 = f1_score(preds.cpu().detach().numpy(), labels.cpu().detach().numpy(),\n                          average='weighted')\n            lossitem = curr_loss.item()\n            \n            f1train.append(f1)\n            losstrain.append(lossitem)\n            \n            curr_loss.backward()\n            optimizer.step()\n            \n            tepoch.set_postfix({'loss': lossitem, 'f1_weighted': f1})\n            \n    scheduler.step()\n    model1.eval()\n    model1.freeze_berts()\n    with tqdm(valloader, unit='batch') as tepoch:\n        for batch in tepoch:\n            text = batch['text']\n            labels = batch['labels'].type(torch.LongTensor).to(device)\n            \n            logits = model1(text).to(device)\n            curr_loss = loss(logits, labels)\n            preds = torch.argmax(sfmax(logits), dim=1, keepdim=True).squeeze()\n            \n            f1 = f1_score(preds.cpu().detach().numpy(), labels.cpu().detach().numpy(),\n                          average='weighted')\n            lossitem = curr_loss.item()\n            \n            f1val.append(f1)\n            lossval.append(lossitem)\n            \n            tepoch.set_postfix({'loss': lossitem, 'f1_weighted': f1})\n    ","metadata":{"execution":{"iopub.status.busy":"2024-04-21T10:38:17.373608Z","iopub.execute_input":"2024-04-21T10:38:17.374344Z","iopub.status.idle":"2024-04-21T12:17:51.423125Z","shell.execute_reply.started":"2024-04-21T10:38:17.374312Z","shell.execute_reply":"2024-04-21T12:17:51.422241Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"100%|██████████| 290/290 [25:08<00:00,  5.20s/batch, loss=0.43, f1_weighted=0.807] \n100%|██████████| 97/97 [08:02<00:00,  4.98s/batch, loss=0.469, f1_weighted=0.881]\n100%|██████████| 290/290 [25:10<00:00,  5.21s/batch, loss=0.32, f1_weighted=0.874] \n100%|██████████| 97/97 [08:02<00:00,  4.98s/batch, loss=0.448, f1_weighted=0.865]\n100%|██████████| 290/290 [25:07<00:00,  5.20s/batch, loss=0.256, f1_weighted=0.86] \n100%|██████████| 97/97 [08:01<00:00,  4.97s/batch, loss=0.435, f1_weighted=0.869]\n","output_type":"stream"}]},{"cell_type":"code","source":"model1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_path = 'mailru_bert_mailru_bert_base.pth'\n\ntorch.save(model1.state_dict(), file_path)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T12:19:22.383759Z","iopub.execute_input":"2024-04-21T12:19:22.384539Z","iopub.status.idle":"2024-04-21T12:19:24.175659Z","shell.execute_reply.started":"2024-04-21T12:19:22.384501Z","shell.execute_reply":"2024-04-21T12:19:24.174683Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    for i in valloader:\n        print(i)\n        break","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_preds = []\nall_labels = []\nwith torch.no_grad():\n    with tqdm(valloader, unit='batch') as tepoch:\n        for batch in tepoch:\n            text = batch['text']\n            labels = batch['labels'].type(torch.LongTensor).to(device)\n\n            logits = model1(text).to(device)\n            curr_loss = loss(logits, labels)\n            preds = sfmax(logits)\n\n            all_preds.extend(preds.cpu().detach().numpy())\n            all_labels.extend(labels.cpu().detach().numpy())","metadata":{"execution":{"iopub.status.busy":"2024-04-21T12:28:08.493305Z","iopub.execute_input":"2024-04-21T12:28:08.493658Z","iopub.status.idle":"2024-04-21T12:36:10.198117Z","shell.execute_reply.started":"2024-04-21T12:28:08.493631Z","shell.execute_reply":"2024-04-21T12:36:10.197033Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stderr","text":"100%|██████████| 97/97 [08:01<00:00,  4.97s/batch]\n","output_type":"stream"}]},{"cell_type":"code","source":"np.save('preds3', np.array(all_preds))","metadata":{"execution":{"iopub.status.busy":"2024-04-21T12:36:40.835362Z","iopub.execute_input":"2024-04-21T12:36:40.835720Z","iopub.status.idle":"2024-04-21T12:36:40.851389Z","shell.execute_reply.started":"2024-04-21T12:36:40.835691Z","shell.execute_reply":"2024-04-21T12:36:40.850368Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"np.save('labels', np.array(all_labels))","metadata":{"execution":{"iopub.status.busy":"2024-04-21T12:37:10.457156Z","iopub.execute_input":"2024-04-21T12:37:10.458055Z","iopub.status.idle":"2024-04-21T12:37:10.463257Z","shell.execute_reply.started":"2024-04-21T12:37:10.458019Z","shell.execute_reply":"2024-04-21T12:37:10.462369Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"np.array(all_preds).shape","metadata":{"execution":{"iopub.status.busy":"2024-04-21T12:40:47.703403Z","iopub.execute_input":"2024-04-21T12:40:47.704289Z","iopub.status.idle":"2024-04-21T12:40:47.719286Z","shell.execute_reply.started":"2024-04-21T12:40:47.704249Z","shell.execute_reply":"2024-04-21T12:40:47.718486Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"(12354, 25)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}